# Text-to-Spatial Audio Parameter Regression

This repository contains the implementation for the ICASSP 2026 paper:
**"Natural Language to Spatial Audio Parameters: Lightweight Deterministic Rendering for Creative Authoring"**

Authors: Seungryeol Paik, Kyogu Lee
Affiliation: Seoul National University

Demo page: https://paiiek.github.io/mmhoa-demo/

---

## ğŸ“ Repository Structure

```
text2hoa/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ final/                      # Production-ready code and models
â”‚   â”œâ”€â”€ models/                 # Best trained models
â”‚   â”‚   â”œâ”€â”€ t2sa_e2e_minilm_stage4f_lastmilefocus.pt  # Main model (33.2Â° AE)
â”‚   â”‚   â”œâ”€â”€ t2sa_minilm_ft_clean_lastmile_ep6.pt      # Alternative checkpoint
â”‚   â”‚   â””â”€â”€ t2sa_e2e_e5s_align.pt                     # E5-small variant
â”‚   â”œâ”€â”€ datasets/               # Final train/valid/test splits
â”‚   â”‚   â”œâ”€â”€ text2spatial_v4_train.jsonl
â”‚   â”‚   â”œâ”€â”€ text2spatial_v4_valid.jsonl
â”‚   â”‚   â”œâ”€â”€ text2spatial_v4_test.jsonl
â”‚   â”‚   â””â”€â”€ tiny.jsonl          # Small test set
â”‚   â””â”€â”€ configs/                # Training and inference scripts
â”‚       â”œâ”€â”€ train_e2e_minilm_v5b_c2f_adamargin_focus_fixed2.py  # Main training
â”‚       â”œâ”€â”€ train_e2e_e5small_v3_align.py                        # E5 training
â”‚       â”œâ”€â”€ eval_lastmile_v4.py                                  # Evaluation
â”‚       â”œâ”€â”€ infer_text2spatial_api.py                            # Inference API
â”‚       â”œâ”€â”€ infer_render.py                                      # Rendering pipeline
â”‚       â””â”€â”€ data.py                                              # Data processing
â”œâ”€â”€ utils/                      # Utility scripts
â”‚   â”œâ”€â”€ check_dataset_coverage.py         # Dataset statistics
â”‚   â”œâ”€â”€ make_dataset_v3_no_subject.py     # Dataset generation
â”‚   â”œâ”€â”€ augment_text_v3_labelaware.py     # Text augmentation
â”‚   â”œâ”€â”€ make_ood_textset_v1.py            # OOD test set creation
â”‚   â”œâ”€â”€ eval_ood_textset.py               # OOD evaluation
â”‚   â”œâ”€â”€ eval_hrtf_robustness.py           # Cross-HRTF testing
â”‚   â”œâ”€â”€ bakeoff_encoders_v1_fix.py        # Encoder comparison
â”‚   â”œâ”€â”€ baseline_rulelex_eval.py          # Rule-based baseline
â”‚   â”œâ”€â”€ gen_fig_*.py                      # Figure generation
â”‚   â””â”€â”€ make_icassp_tables.py             # Paper tables
â”œâ”€â”€ renderer/                   # Spatial audio rendering backend
â”‚   â”œâ”€â”€ hrtf/kemar.sofa         # HRTF data (KEMAR)
â”‚   â””â”€â”€ mos_questions_*/        # MOS listening test stimuli
â”œâ”€â”€ v2/                         # Improved pipeline (LoRA experiments)
â”œâ”€â”€ v3/                         # Latest experiments
â”œâ”€â”€ emotion/                    # Emotion-based spatial audio
â”œâ”€â”€ archive/                    # Historical experiments (not for production)
â”‚   â”œâ”€â”€ intermediate_models/    # Old checkpoints
â”‚   â”œâ”€â”€ intermediate_datasets/  # Dataset versions
â”‚   â”œâ”€â”€ intermediate_scripts/   # Training variants
â”‚   â””â”€â”€ old_eval/               # Previous evaluation scripts
â”œâ”€â”€ cache_*.pt                  # Pre-computed text embeddings
â”œâ”€â”€ metrics_*.json              # Evaluation results
â”œâ”€â”€ commands.txt                # Example commands
â””â”€â”€ text2spatial_v4_stats.json  # Dataset normalization stats
```

---

## ğŸš€ Quick Start

### 1. **Installation**

```bash
pip install torch transformers sofa-tools librosa soundfile pydub
```

### 2. **Inference**

```bash
cd final/configs

# Single text input
python infer_text2spatial_api.py \
  --ckpt ../models/t2sa_e2e_minilm_stage4f_lastmilefocus.pt \
  --text "ì˜¤ë¥¸ìª½ ì•ì—ì„œ ì¡°ê¸ˆì”© ì„ ëª…í•´ì ¸"

# Multiple inputs (Korean + English)
python infer_text2spatial_api.py \
  --ckpt ../models/t2sa_e2e_minilm_stage4f_lastmilefocus.pt \
  --text "ì•ì˜¤ë¥¸ìª½ì—ì„œ ë¹ ë¥´ê²Œ ìŠ¤ì³ê°€" "approaches between right and back, softly"
```

### 3. **Rendering**

```bash
# Binaural rendering with HRTF
python infer_render.py \
  --text "ì™¼ìª½ ë’¤ì—ì„œ ì²œì²œíˆ ë‹¤ê°€ì™€" \
  --mono_audio input.wav \
  --output output_binaural.wav \
  --hrtf ../renderer/hrtf/kemar.sofa
```

---

## ğŸ“Š Dataset

**Total samples:** 17,151 (after augmentation)
- **Train:** 15,000
- **Validation:** 1,700
- **Languages:** Korean (59%), English (41%)

**Spatial parameters:**
- Azimuth (sin/cos representation)
- Elevation (radians, Â±60Â°)
- Distance (0.6â€“6.0 m, log-scaled)
- Spread (5â€“120Â°)
- Reverberation (wet mix, 0â€“1)
- Gain (dB)
- Room descriptor (DRR or RT60)

**Data format (JSONL):**
```json
{
  "text": "ì˜¤ë¥¸ìª½ ì•ì—ì„œ ì¡°ê¸ˆì”© ì„ ëª…í•´ì ¸",
  "lang": "ko",
  "az_sc": [1.0, 0.0],
  "el_rad": 0.0,
  "dist_m": 1.217,
  "spread_deg": 18.2,
  "wet_mix": 0.23,
  "room_depth": {"drr_db": 8.68},
  "gain_db": -4.98
}
```

---

## ğŸ¯ Model Architecture

**Encoder:** `paraphrase-multilingual-MiniLM-L12-v2`
- Fine-tuned with BitFit (bias + LayerNorm only)
- Last 2 layers unfrozen

**Regression Head:**
```
Linear(384 â†’ 768) â†’ ReLU â†’ Dropout(0.1)
â†’ Linear(768 â†’ 384) â†’ ReLU
â†’ Linear(384 â†’ 8)  # [sin(az), cos(az), el, dist, spread, wet, gain, room]
```

**Training objectives:**
1. Angular-margin loss (ArcFace for 12-bin azimuth classification)
2. MAE for continuous parameters (elevation, distance, spread, wet, gain, room)
3. Adaptive margins (per-bin adjustment based on validation error)
4. Directional contrast loss (metric learning for spatial embeddings)
5. KNN adjustment (nearest-neighbor smoothing)

---

## ğŸ“ˆ Results

**Main paper results (33.2Â° angular error):**
- Model: `t2sa_e2e_minilm_stage4f_lastmilefocus.pt`
- Dataset: `text2spatial_v4_train.jsonl`
- Encoder: MiniLM-L12-v2

**Ablation study:**
| Configuration | Angular Error (Â°) |
|--------------|------------------|
| Full model | **33.2** |
| -- Angular-margin | 41.0 |
| -- Adaptive margins | 38.7 |
| -- Directional focus | 36.8 |
| -- KNN adjustment | 37.5 |
| E5 encoder | 38.2 |

**Baselines:**
- Rule-based (keyword matching): 71.0Â°
- Linear (SBERT): 61.8Â°
- Linear (E5): 76.8Â°

**MOS (25 participants):**
- Overall preference: 4.02 Â± 0.64 (5-point scale)
- Localization clarity: 4.28 Â± 0.60
- Textâ€“spatial fit: 4.12 Â± 0.63
- Naturalness: 3.96 Â± 0.64

---

## ğŸ”¬ Evaluation

```bash
cd final/configs

# Standard evaluation
python eval_lastmile_v4.py \
  --data ../datasets/text2spatial_v4_test.jsonl \
  --ckpt ../models/t2sa_e2e_minilm_stage4f_lastmilefocus.pt

# OOD robustness
cd ../../utils
python eval_ood_textset.py \
  --ckpt ../final/models/t2sa_e2e_minilm_stage4f_lastmilefocus.pt

# Cross-HRTF robustness
python eval_hrtf_robustness.py \
  --cipic_path /path/to/cipic.sofa \
  --kemar_path ../renderer/hrtf/kemar.sofa
```

---

## ğŸ“ Training

```bash
cd final/configs

# Main training (reproduces paper results)
python train_e2e_minilm_v5b_c2f_adamargin_focus_fixed2.py \
  --data ../datasets/text2spatial_v4_train.jsonl \
  --epochs 16 --bsz 96 --lr_head 3e-4 --lr_enc 5e-6 \
  --save ../models/my_model.pt

# E5-small variant
python train_e2e_e5small_v3_align.py \
  --data ../datasets/text2spatial_v4_train.jsonl \
  --epochs 28 --bsz 128 --bitfit --unfreeze_last_n 2
```

---

## ğŸ¨ Rendering Backend

The deterministic renderer supports:
- **Stereo panning** (intensity-based)
- **Binaural** (HRTF convolution with SOFA datasets)
- **First-Order Ambisonics (FOA)** (B-format output)

**OSC Export:**
Parameters can be streamed to SpatRevolution or other DAWs via OSC.

---

## ğŸ“š Citation

```bibtex
@inproceedings{paik2026text2spatial,
  title={Natural Language to Spatial Audio Parameters: Lightweight Deterministic Rendering for Creative Authoring},
  author={Paik, Seungryeol and Lee, Kyogu},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2026}
}
```

---

## ğŸ› ï¸ Folder Contents Summary

- **`final/`**: Production code, models, and datasets
- **`utils/`**: Helper scripts for data processing, evaluation, and figure generation
- **`renderer/`**: Spatial audio rendering engine and MOS test stimuli
- **`v2/`, `v3/`**: Experimental pipelines (LoRA, multi-task learning)
- **`emotion/`**: Emotion-conditioned spatial audio
- **`archive/`**: Historical experiments (not recommended for reproduction)

---

## ğŸ”— Links

- **Demo page:** https://paiiek.github.io/mmhoa-demo/
- **Paper:** [To be added after publication]
- **HRTF datasets:**
  - CIPIC: https://www.ece.ucdavis.edu/cipic/spatial-sound/hrtf-data/
  - KEMAR: https://sound.media.mit.edu/resources/KEMAR.html

---

## ğŸ“§ Contact

For questions or collaboration:
- **Seungryeol Paik:** paiiek@snu.ac.kr
- **Kyogu Lee:** kglee@snu.ac.kr

---

## âš–ï¸ License

This code is released for academic research purposes. Commercial use requires permission from the authors.

---

## ğŸ™ Acknowledgments

- HRTF datasets: CIPIC, KEMAR (MIT Media Lab)
- Pre-trained encoders: Sentence-Transformers, E5 (Microsoft)
- Supported by Seoul National University AI Institute
