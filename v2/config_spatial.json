{
  "encoder": "xlm-roberta-base",
  "batch_size": 8,
  "accum_steps": 2,
  "epochs": 15,
  "lr_enc": 2e-5,
  "lr_head": 1e-3,
  "warmup_ratio": 0.1,
  "freeze_epochs": 1,
  "augment_prob": 0.25,
  "loss": "huber",
  "huber_delta": 1.0,
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05
}